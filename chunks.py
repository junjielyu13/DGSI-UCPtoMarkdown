import os
import markdown
from bs4 import BeautifulSoup
from langchain.text_splitter import RecursiveCharacterTextSplitter
from sentence_transformers import SentenceTransformer
import chromadb

def extract_text_from_markdown(md_file):
    """ è¯»å– Markdown æ–‡ä»¶å¹¶æå–çº¯æ–‡æœ¬ """
    with open(md_file, "r", encoding="utf-8") as f:
        md_content = f.read()
    
    # è½¬æ¢ä¸º HTML å¹¶å»é™¤æ ¼å¼
    html_content = markdown.markdown(md_content)
    soup = BeautifulSoup(html_content, "html.parser")
    return soup.get_text()

def load_markdown_files(folder):
    """ é€’å½’è¯»å–æ–‡ä»¶å¤¹åŠå…¶å­æ–‡ä»¶å¤¹ä¸­çš„æ‰€æœ‰ Markdown æ–‡ä»¶ """
    all_texts = []
    
    for root, _, files in os.walk(folder):  # ä½¿ç”¨ os.walk é€’å½’éå†æ‰€æœ‰å­æ–‡ä»¶å¤¹
        for filename in files:
            if filename.endswith(".md"):
                file_path = os.path.join(root, filename)
                text = extract_text_from_markdown(file_path)
                all_texts.append(text)
                
    return all_texts

def chunk_texts(texts, chunk_size=500, chunk_overlap=50):
    """ æ‹†åˆ†æ–‡æœ¬ä¸º Chunks """
    text_splitter = RecursiveCharacterTextSplitter(chunk_size=chunk_size, chunk_overlap=chunk_overlap)
    chunks = []
    for text in texts:
        chunks.extend(text_splitter.split_text(text))
    return chunks

def compute_embeddings(chunks):
    """ ä½¿ç”¨æœ¬åœ°æ¨¡å‹è®¡ç®—åµŒå…¥ """
    model = SentenceTransformer("all-MiniLM-L6-v2")
    embeddings = model.encode(chunks, convert_to_numpy=True)
    return embeddings, model

def store_in_chromadb(chunks, embeddings):
    """ å­˜å…¥ ChromaDB """
    chroma_client = chromadb.PersistentClient(path="./chroma_db")
    collection = chroma_client.get_or_create_collection(name="markdown_docs")
    
    for i, (chunk, embedding) in enumerate(zip(chunks, embeddings)):
        collection.add(
            ids=[str(i)],
            embeddings=[embedding.tolist()],
            metadatas=[{"source": f"Markdown Chunk {i}"}],
            documents=[chunk]
        )
    print("æ•°æ®å­˜å…¥ ChromaDB æˆåŠŸï¼")

def query_chromadb(query_text, model, top_n=3):
    """ åœ¨ ChromaDB è¿›è¡Œç›¸ä¼¼æ€§æœç´¢ """
    chroma_client = chromadb.PersistentClient(path="./chroma_db")
    collection = chroma_client.get_collection(name="markdown_docs")
    query_embedding = model.encode([query_text])[0].tolist()
    results = collection.query(query_embeddings=[query_embedding], n_results=top_n)
    
    for doc, score in zip(results["documents"][0], results["distances"][0]):
        print(f"ç›¸å…³ Chunk: {doc} (ç›¸ä¼¼åº¦: {score})")

if __name__ == "__main__":
    folder_path = "./markdown_pages"  # ä½ çš„ Markdown æ–‡ä»¶å¤¹è·¯å¾„
    texts = load_markdown_files(folder_path)
    chunks = chunk_texts(texts)
    embeddings, model = compute_embeddings(chunks)
    store_in_chromadb(chunks, embeddings)
    
    # æµ‹è¯•æœç´¢
    query_text = "ä»€ä¹ˆæ˜¯ ChromaDBï¼Ÿ"
    print("\nğŸ” æœç´¢ç»“æœ:")
    query_chromadb(query_text, model)